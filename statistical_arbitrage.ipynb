{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from hyperopt import hp\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n",
      "2.1.2+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8902\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"gpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('CUDA version:', torch.version.cuda)\n",
    "print('cuDNN version:', torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ADSK</th>\n",
       "      <th>ORCL</th>\n",
       "      <th>PTC</th>\n",
       "      <th>CDNS</th>\n",
       "      <th>FICO</th>\n",
       "      <th>CRM</th>\n",
       "      <th>SNPS</th>\n",
       "      <th>INTU</th>\n",
       "      <th>TYL</th>\n",
       "      <th>...</th>\n",
       "      <th>NOW</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>PANW</th>\n",
       "      <th>CDW</th>\n",
       "      <th>NTAP</th>\n",
       "      <th>STX</th>\n",
       "      <th>HPQ</th>\n",
       "      <th>WDC</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>HPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>207.149994</td>\n",
       "      <td>86.258675</td>\n",
       "      <td>120.389999</td>\n",
       "      <td>207.880005</td>\n",
       "      <td>696.880005</td>\n",
       "      <td>188.679993</td>\n",
       "      <td>374.359985</td>\n",
       "      <td>419.189697</td>\n",
       "      <td>329.570007</td>\n",
       "      <td>...</td>\n",
       "      <td>445.459991</td>\n",
       "      <td>272.053467</td>\n",
       "      <td>194.490005</td>\n",
       "      <td>192.704956</td>\n",
       "      <td>62.480740</td>\n",
       "      <td>60.264462</td>\n",
       "      <td>27.854996</td>\n",
       "      <td>35.470001</td>\n",
       "      <td>158.636475</td>\n",
       "      <td>14.427283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>2023-03-22</td>\n",
       "      <td>198.380005</td>\n",
       "      <td>86.573853</td>\n",
       "      <td>118.570000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>678.700012</td>\n",
       "      <td>186.509995</td>\n",
       "      <td>371.350006</td>\n",
       "      <td>411.422363</td>\n",
       "      <td>320.709991</td>\n",
       "      <td>...</td>\n",
       "      <td>433.510010</td>\n",
       "      <td>270.572845</td>\n",
       "      <td>190.889999</td>\n",
       "      <td>188.929932</td>\n",
       "      <td>59.911034</td>\n",
       "      <td>58.731163</td>\n",
       "      <td>27.115051</td>\n",
       "      <td>34.700001</td>\n",
       "      <td>157.192352</td>\n",
       "      <td>14.114285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>2023-03-23</td>\n",
       "      <td>201.139999</td>\n",
       "      <td>86.455650</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>680.849976</td>\n",
       "      <td>187.440002</td>\n",
       "      <td>379.399994</td>\n",
       "      <td>429.579315</td>\n",
       "      <td>322.549988</td>\n",
       "      <td>...</td>\n",
       "      <td>440.470001</td>\n",
       "      <td>275.908997</td>\n",
       "      <td>193.889999</td>\n",
       "      <td>189.346069</td>\n",
       "      <td>59.891563</td>\n",
       "      <td>59.779240</td>\n",
       "      <td>26.988483</td>\n",
       "      <td>35.270000</td>\n",
       "      <td>158.287888</td>\n",
       "      <td>13.957786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>200.220001</td>\n",
       "      <td>86.682182</td>\n",
       "      <td>121.699997</td>\n",
       "      <td>204.479996</td>\n",
       "      <td>690.940002</td>\n",
       "      <td>190.059998</td>\n",
       "      <td>376.559998</td>\n",
       "      <td>426.678986</td>\n",
       "      <td>329.480011</td>\n",
       "      <td>...</td>\n",
       "      <td>432.899994</td>\n",
       "      <td>278.800659</td>\n",
       "      <td>191.550003</td>\n",
       "      <td>189.950470</td>\n",
       "      <td>59.434082</td>\n",
       "      <td>59.818062</td>\n",
       "      <td>26.988483</td>\n",
       "      <td>35.259998</td>\n",
       "      <td>159.602585</td>\n",
       "      <td>13.918660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>198.750000</td>\n",
       "      <td>88.780052</td>\n",
       "      <td>123.169998</td>\n",
       "      <td>205.270004</td>\n",
       "      <td>688.630005</td>\n",
       "      <td>191.259995</td>\n",
       "      <td>373.920013</td>\n",
       "      <td>424.066650</td>\n",
       "      <td>332.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>431.309998</td>\n",
       "      <td>274.637054</td>\n",
       "      <td>192.529999</td>\n",
       "      <td>189.910843</td>\n",
       "      <td>60.018108</td>\n",
       "      <td>60.177120</td>\n",
       "      <td>27.153997</td>\n",
       "      <td>35.209999</td>\n",
       "      <td>157.640533</td>\n",
       "      <td>14.681595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>242.759995</td>\n",
       "      <td>105.790077</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>275.820007</td>\n",
       "      <td>1168.880005</td>\n",
       "      <td>266.339996</td>\n",
       "      <td>524.460022</td>\n",
       "      <td>623.131348</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>697.549988</td>\n",
       "      <td>374.579987</td>\n",
       "      <td>298.209991</td>\n",
       "      <td>226.630005</td>\n",
       "      <td>88.882095</td>\n",
       "      <td>85.160004</td>\n",
       "      <td>30.180000</td>\n",
       "      <td>52.660000</td>\n",
       "      <td>193.600006</td>\n",
       "      <td>17.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>245.070007</td>\n",
       "      <td>105.780121</td>\n",
       "      <td>175.309998</td>\n",
       "      <td>274.959991</td>\n",
       "      <td>1164.619995</td>\n",
       "      <td>266.220001</td>\n",
       "      <td>520.250000</td>\n",
       "      <td>623.910156</td>\n",
       "      <td>416.410004</td>\n",
       "      <td>...</td>\n",
       "      <td>701.229980</td>\n",
       "      <td>374.660004</td>\n",
       "      <td>300.820007</td>\n",
       "      <td>228.729996</td>\n",
       "      <td>88.862213</td>\n",
       "      <td>85.870003</td>\n",
       "      <td>30.410000</td>\n",
       "      <td>52.759998</td>\n",
       "      <td>193.050003</td>\n",
       "      <td>17.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>245.110001</td>\n",
       "      <td>105.531090</td>\n",
       "      <td>175.720001</td>\n",
       "      <td>274.640015</td>\n",
       "      <td>1170.609985</td>\n",
       "      <td>266.720001</td>\n",
       "      <td>518.099976</td>\n",
       "      <td>628.173767</td>\n",
       "      <td>415.600006</td>\n",
       "      <td>...</td>\n",
       "      <td>703.760010</td>\n",
       "      <td>374.070007</td>\n",
       "      <td>297.500000</td>\n",
       "      <td>228.550003</td>\n",
       "      <td>88.086731</td>\n",
       "      <td>85.680000</td>\n",
       "      <td>30.240000</td>\n",
       "      <td>52.419998</td>\n",
       "      <td>193.149994</td>\n",
       "      <td>17.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>244.910004</td>\n",
       "      <td>105.839890</td>\n",
       "      <td>175.919998</td>\n",
       "      <td>273.239990</td>\n",
       "      <td>1169.339966</td>\n",
       "      <td>265.579987</td>\n",
       "      <td>517.409973</td>\n",
       "      <td>627.075439</td>\n",
       "      <td>418.290009</td>\n",
       "      <td>...</td>\n",
       "      <td>702.460022</td>\n",
       "      <td>375.279999</td>\n",
       "      <td>295.579987</td>\n",
       "      <td>228.699997</td>\n",
       "      <td>87.698990</td>\n",
       "      <td>86.790001</td>\n",
       "      <td>30.180000</td>\n",
       "      <td>52.930000</td>\n",
       "      <td>193.580002</td>\n",
       "      <td>17.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>243.479996</td>\n",
       "      <td>105.023056</td>\n",
       "      <td>174.960007</td>\n",
       "      <td>272.369995</td>\n",
       "      <td>1164.010010</td>\n",
       "      <td>263.140015</td>\n",
       "      <td>514.909973</td>\n",
       "      <td>624.089966</td>\n",
       "      <td>418.119995</td>\n",
       "      <td>...</td>\n",
       "      <td>706.489990</td>\n",
       "      <td>376.040009</td>\n",
       "      <td>294.880005</td>\n",
       "      <td>227.320007</td>\n",
       "      <td>87.649284</td>\n",
       "      <td>85.370003</td>\n",
       "      <td>30.090000</td>\n",
       "      <td>52.369999</td>\n",
       "      <td>192.529999</td>\n",
       "      <td>16.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        ADSK        ORCL         PTC        CDNS         FICO  \\\n",
       "809   2023-03-21  207.149994   86.258675  120.389999  207.880005   696.880005   \n",
       "810   2023-03-22  198.380005   86.573853  118.570000  204.000000   678.700012   \n",
       "811   2023-03-23  201.139999   86.455650  119.980003  207.000000   680.849976   \n",
       "812   2023-03-24  200.220001   86.682182  121.699997  204.479996   690.940002   \n",
       "813   2023-03-27  198.750000   88.780052  123.169998  205.270004   688.630005   \n",
       "...          ...         ...         ...         ...         ...          ...   \n",
       "1001  2023-12-22  242.759995  105.790077  173.970001  275.820007  1168.880005   \n",
       "1002  2023-12-26  245.070007  105.780121  175.309998  274.959991  1164.619995   \n",
       "1003  2023-12-27  245.110001  105.531090  175.720001  274.640015  1170.609985   \n",
       "1004  2023-12-28  244.910004  105.839890  175.919998  273.239990  1169.339966   \n",
       "1005  2023-12-29  243.479996  105.023056  174.960007  272.369995  1164.010010   \n",
       "\n",
       "             CRM        SNPS        INTU         TYL  ...         NOW  \\\n",
       "809   188.679993  374.359985  419.189697  329.570007  ...  445.459991   \n",
       "810   186.509995  371.350006  411.422363  320.709991  ...  433.510010   \n",
       "811   187.440002  379.399994  429.579315  322.549988  ...  440.470001   \n",
       "812   190.059998  376.559998  426.678986  329.480011  ...  432.899994   \n",
       "813   191.259995  373.920013  424.066650  332.250000  ...  431.309998   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "1001  266.339996  524.460022  623.131348  415.000000  ...  697.549988   \n",
       "1002  266.220001  520.250000  623.910156  416.410004  ...  701.229980   \n",
       "1003  266.720001  518.099976  628.173767  415.600006  ...  703.760010   \n",
       "1004  265.579987  517.409973  627.075439  418.290009  ...  702.460022   \n",
       "1005  263.140015  514.909973  624.089966  418.119995  ...  706.489990   \n",
       "\n",
       "            MSFT        PANW         CDW       NTAP        STX        HPQ  \\\n",
       "809   272.053467  194.490005  192.704956  62.480740  60.264462  27.854996   \n",
       "810   270.572845  190.889999  188.929932  59.911034  58.731163  27.115051   \n",
       "811   275.908997  193.889999  189.346069  59.891563  59.779240  26.988483   \n",
       "812   278.800659  191.550003  189.950470  59.434082  59.818062  26.988483   \n",
       "813   274.637054  192.529999  189.910843  60.018108  60.177120  27.153997   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "1001  374.579987  298.209991  226.630005  88.882095  85.160004  30.180000   \n",
       "1002  374.660004  300.820007  228.729996  88.862213  85.870003  30.410000   \n",
       "1003  374.070007  297.500000  228.550003  88.086731  85.680000  30.240000   \n",
       "1004  375.279999  295.579987  228.699997  87.698990  86.790001  30.180000   \n",
       "1005  376.040009  294.880005  227.320007  87.649284  85.370003  30.090000   \n",
       "\n",
       "            WDC        AAPL        HPE  \n",
       "809   35.470001  158.636475  14.427283  \n",
       "810   34.700001  157.192352  14.114285  \n",
       "811   35.270000  158.287888  13.957786  \n",
       "812   35.259998  159.602585  13.918660  \n",
       "813   35.209999  157.640533  14.681595  \n",
       "...         ...         ...        ...  \n",
       "1001  52.660000  193.600006  17.090000  \n",
       "1002  52.759998  193.050003  17.230000  \n",
       "1003  52.419998  193.149994  17.090000  \n",
       "1004  52.930000  193.580002  17.170000  \n",
       "1005  52.369999  192.529999  16.980000  \n",
       "\n",
       "[197 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Technology_Firm_Stock_Price.csv\")\n",
    "data = data.sort_values(by=\"Date\")\n",
    "data.iloc[-197:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADSK</th>\n",
       "      <th>ORCL</th>\n",
       "      <th>PTC</th>\n",
       "      <th>CDNS</th>\n",
       "      <th>FICO</th>\n",
       "      <th>CRM</th>\n",
       "      <th>SNPS</th>\n",
       "      <th>INTU</th>\n",
       "      <th>TYL</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>...</th>\n",
       "      <th>NOW</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>PANW</th>\n",
       "      <th>CDW</th>\n",
       "      <th>NTAP</th>\n",
       "      <th>STX</th>\n",
       "      <th>HPQ</th>\n",
       "      <th>WDC</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>HPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187.830002</td>\n",
       "      <td>50.359100</td>\n",
       "      <td>76.370003</td>\n",
       "      <td>71.440002</td>\n",
       "      <td>382.920013</td>\n",
       "      <td>166.990005</td>\n",
       "      <td>142.869995</td>\n",
       "      <td>258.836151</td>\n",
       "      <td>306.239990</td>\n",
       "      <td>334.429993</td>\n",
       "      <td>...</td>\n",
       "      <td>291.239990</td>\n",
       "      <td>154.779541</td>\n",
       "      <td>78.470001</td>\n",
       "      <td>137.363739</td>\n",
       "      <td>55.579060</td>\n",
       "      <td>51.156185</td>\n",
       "      <td>18.257439</td>\n",
       "      <td>64.771545</td>\n",
       "      <td>73.152641</td>\n",
       "      <td>14.081444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184.949997</td>\n",
       "      <td>50.181740</td>\n",
       "      <td>75.430000</td>\n",
       "      <td>70.419998</td>\n",
       "      <td>381.920013</td>\n",
       "      <td>166.169998</td>\n",
       "      <td>141.759995</td>\n",
       "      <td>257.105133</td>\n",
       "      <td>306.670013</td>\n",
       "      <td>331.809998</td>\n",
       "      <td>...</td>\n",
       "      <td>291.100006</td>\n",
       "      <td>152.852264</td>\n",
       "      <td>78.943336</td>\n",
       "      <td>135.712097</td>\n",
       "      <td>54.631645</td>\n",
       "      <td>49.811756</td>\n",
       "      <td>18.029110</td>\n",
       "      <td>63.774597</td>\n",
       "      <td>72.441467</td>\n",
       "      <td>13.733110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187.119995</td>\n",
       "      <td>50.443100</td>\n",
       "      <td>76.269997</td>\n",
       "      <td>70.849998</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>173.449997</td>\n",
       "      <td>141.979996</td>\n",
       "      <td>259.079346</td>\n",
       "      <td>310.209991</td>\n",
       "      <td>333.709991</td>\n",
       "      <td>...</td>\n",
       "      <td>292.869995</td>\n",
       "      <td>153.247345</td>\n",
       "      <td>80.086670</td>\n",
       "      <td>134.690536</td>\n",
       "      <td>54.498821</td>\n",
       "      <td>49.219864</td>\n",
       "      <td>18.099367</td>\n",
       "      <td>62.550629</td>\n",
       "      <td>73.018684</td>\n",
       "      <td>13.689568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187.500000</td>\n",
       "      <td>50.555126</td>\n",
       "      <td>75.919998</td>\n",
       "      <td>71.070000</td>\n",
       "      <td>388.489990</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>142.960007</td>\n",
       "      <td>259.137634</td>\n",
       "      <td>311.269989</td>\n",
       "      <td>333.390015</td>\n",
       "      <td>...</td>\n",
       "      <td>292.910004</td>\n",
       "      <td>151.850082</td>\n",
       "      <td>80.546669</td>\n",
       "      <td>134.537781</td>\n",
       "      <td>54.853001</td>\n",
       "      <td>49.811756</td>\n",
       "      <td>18.143276</td>\n",
       "      <td>66.785164</td>\n",
       "      <td>72.675278</td>\n",
       "      <td>13.619902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189.949997</td>\n",
       "      <td>50.752010</td>\n",
       "      <td>76.980003</td>\n",
       "      <td>72.279999</td>\n",
       "      <td>391.329987</td>\n",
       "      <td>177.330002</td>\n",
       "      <td>145.860001</td>\n",
       "      <td>265.371277</td>\n",
       "      <td>310.989990</td>\n",
       "      <td>337.869995</td>\n",
       "      <td>...</td>\n",
       "      <td>295.649994</td>\n",
       "      <td>154.268814</td>\n",
       "      <td>79.523331</td>\n",
       "      <td>134.337265</td>\n",
       "      <td>55.906673</td>\n",
       "      <td>50.200710</td>\n",
       "      <td>18.380386</td>\n",
       "      <td>67.752487</td>\n",
       "      <td>73.844353</td>\n",
       "      <td>13.724401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>242.759995</td>\n",
       "      <td>105.790077</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>275.820007</td>\n",
       "      <td>1168.880005</td>\n",
       "      <td>266.339996</td>\n",
       "      <td>524.460022</td>\n",
       "      <td>623.131348</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>598.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>697.549988</td>\n",
       "      <td>374.579987</td>\n",
       "      <td>298.209991</td>\n",
       "      <td>226.630005</td>\n",
       "      <td>88.882095</td>\n",
       "      <td>85.160004</td>\n",
       "      <td>30.180000</td>\n",
       "      <td>52.660000</td>\n",
       "      <td>193.600006</td>\n",
       "      <td>17.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>245.070007</td>\n",
       "      <td>105.780121</td>\n",
       "      <td>175.309998</td>\n",
       "      <td>274.959991</td>\n",
       "      <td>1164.619995</td>\n",
       "      <td>266.220001</td>\n",
       "      <td>520.250000</td>\n",
       "      <td>623.910156</td>\n",
       "      <td>416.410004</td>\n",
       "      <td>598.260010</td>\n",
       "      <td>...</td>\n",
       "      <td>701.229980</td>\n",
       "      <td>374.660004</td>\n",
       "      <td>300.820007</td>\n",
       "      <td>228.729996</td>\n",
       "      <td>88.862213</td>\n",
       "      <td>85.870003</td>\n",
       "      <td>30.410000</td>\n",
       "      <td>52.759998</td>\n",
       "      <td>193.050003</td>\n",
       "      <td>17.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>245.110001</td>\n",
       "      <td>105.531090</td>\n",
       "      <td>175.720001</td>\n",
       "      <td>274.640015</td>\n",
       "      <td>1170.609985</td>\n",
       "      <td>266.720001</td>\n",
       "      <td>518.099976</td>\n",
       "      <td>628.173767</td>\n",
       "      <td>415.600006</td>\n",
       "      <td>596.080017</td>\n",
       "      <td>...</td>\n",
       "      <td>703.760010</td>\n",
       "      <td>374.070007</td>\n",
       "      <td>297.500000</td>\n",
       "      <td>228.550003</td>\n",
       "      <td>88.086731</td>\n",
       "      <td>85.680000</td>\n",
       "      <td>30.240000</td>\n",
       "      <td>52.419998</td>\n",
       "      <td>193.149994</td>\n",
       "      <td>17.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>244.910004</td>\n",
       "      <td>105.839890</td>\n",
       "      <td>175.919998</td>\n",
       "      <td>273.239990</td>\n",
       "      <td>1169.339966</td>\n",
       "      <td>265.579987</td>\n",
       "      <td>517.409973</td>\n",
       "      <td>627.075439</td>\n",
       "      <td>418.290009</td>\n",
       "      <td>595.520020</td>\n",
       "      <td>...</td>\n",
       "      <td>702.460022</td>\n",
       "      <td>375.279999</td>\n",
       "      <td>295.579987</td>\n",
       "      <td>228.699997</td>\n",
       "      <td>87.698990</td>\n",
       "      <td>86.790001</td>\n",
       "      <td>30.180000</td>\n",
       "      <td>52.930000</td>\n",
       "      <td>193.580002</td>\n",
       "      <td>17.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>243.479996</td>\n",
       "      <td>105.023056</td>\n",
       "      <td>174.960007</td>\n",
       "      <td>272.369995</td>\n",
       "      <td>1164.010010</td>\n",
       "      <td>263.140015</td>\n",
       "      <td>514.909973</td>\n",
       "      <td>624.089966</td>\n",
       "      <td>418.119995</td>\n",
       "      <td>596.599976</td>\n",
       "      <td>...</td>\n",
       "      <td>706.489990</td>\n",
       "      <td>376.040009</td>\n",
       "      <td>294.880005</td>\n",
       "      <td>227.320007</td>\n",
       "      <td>87.649284</td>\n",
       "      <td>85.370003</td>\n",
       "      <td>30.090000</td>\n",
       "      <td>52.369999</td>\n",
       "      <td>192.529999</td>\n",
       "      <td>16.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1006 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ADSK        ORCL         PTC        CDNS         FICO         CRM  \\\n",
       "0     187.830002   50.359100   76.370003   71.440002   382.920013  166.990005   \n",
       "1     184.949997   50.181740   75.430000   70.419998   381.920013  166.169998   \n",
       "2     187.119995   50.443100   76.269997   70.849998   384.000000  173.449997   \n",
       "3     187.500000   50.555126   75.919998   71.070000   388.489990  176.000000   \n",
       "4     189.949997   50.752010   76.980003   72.279999   391.329987  177.330002   \n",
       "...          ...         ...         ...         ...          ...         ...   \n",
       "1001  242.759995  105.790077  173.970001  275.820007  1168.880005  266.339996   \n",
       "1002  245.070007  105.780121  175.309998  274.959991  1164.619995  266.220001   \n",
       "1003  245.110001  105.531090  175.720001  274.640015  1170.609985  266.720001   \n",
       "1004  244.910004  105.839890  175.919998  273.239990  1169.339966  265.579987   \n",
       "1005  243.479996  105.023056  174.960007  272.369995  1164.010010  263.140015   \n",
       "\n",
       "            SNPS        INTU         TYL        ADBE  ...         NOW  \\\n",
       "0     142.869995  258.836151  306.239990  334.429993  ...  291.239990   \n",
       "1     141.759995  257.105133  306.670013  331.809998  ...  291.100006   \n",
       "2     141.979996  259.079346  310.209991  333.709991  ...  292.869995   \n",
       "3     142.960007  259.137634  311.269989  333.390015  ...  292.910004   \n",
       "4     145.860001  265.371277  310.989990  337.869995  ...  295.649994   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "1001  524.460022  623.131348  415.000000  598.750000  ...  697.549988   \n",
       "1002  520.250000  623.910156  416.410004  598.260010  ...  701.229980   \n",
       "1003  518.099976  628.173767  415.600006  596.080017  ...  703.760010   \n",
       "1004  517.409973  627.075439  418.290009  595.520020  ...  702.460022   \n",
       "1005  514.909973  624.089966  418.119995  596.599976  ...  706.489990   \n",
       "\n",
       "            MSFT        PANW         CDW       NTAP        STX        HPQ  \\\n",
       "0     154.779541   78.470001  137.363739  55.579060  51.156185  18.257439   \n",
       "1     152.852264   78.943336  135.712097  54.631645  49.811756  18.029110   \n",
       "2     153.247345   80.086670  134.690536  54.498821  49.219864  18.099367   \n",
       "3     151.850082   80.546669  134.537781  54.853001  49.811756  18.143276   \n",
       "4     154.268814   79.523331  134.337265  55.906673  50.200710  18.380386   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "1001  374.579987  298.209991  226.630005  88.882095  85.160004  30.180000   \n",
       "1002  374.660004  300.820007  228.729996  88.862213  85.870003  30.410000   \n",
       "1003  374.070007  297.500000  228.550003  88.086731  85.680000  30.240000   \n",
       "1004  375.279999  295.579987  228.699997  87.698990  86.790001  30.180000   \n",
       "1005  376.040009  294.880005  227.320007  87.649284  85.370003  30.090000   \n",
       "\n",
       "            WDC        AAPL        HPE  \n",
       "0     64.771545   73.152641  14.081444  \n",
       "1     63.774597   72.441467  13.733110  \n",
       "2     62.550629   73.018684  13.689568  \n",
       "3     66.785164   72.675278  13.619902  \n",
       "4     67.752487   73.844353  13.724401  \n",
       "...         ...         ...        ...  \n",
       "1001  52.660000  193.600006  17.090000  \n",
       "1002  52.759998  193.050003  17.230000  \n",
       "1003  52.419998  193.149994  17.090000  \n",
       "1004  52.930000  193.580002  17.170000  \n",
       "1005  52.369999  192.529999  16.980000  \n",
       "\n",
       "[1006 rows x 64 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=[\"Date\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.251273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.237310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.247831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.249673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.261550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>0.517574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.528773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>0.528967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.527997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>0.521065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1006 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0     0.251273\n",
       "1     0.237310\n",
       "2     0.247831\n",
       "3     0.249673\n",
       "4     0.261550\n",
       "...        ...\n",
       "1001  0.517574\n",
       "1002  0.528773\n",
       "1003  0.528967\n",
       "1004  0.527997\n",
       "1005  0.521065\n",
       "\n",
       "[1006 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data_reshape = data[\"ADSK\"].values.reshape(-1,1)\n",
    "scaled_data = pd.DataFrame(scaler.fit_transform(data_reshape))\n",
    "\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADSK is added to the dictionary!\n",
      "ORCL is added to the dictionary!\n",
      "PTC is added to the dictionary!\n",
      "CDNS is added to the dictionary!\n",
      "FICO is added to the dictionary!\n",
      "CRM is added to the dictionary!\n",
      "SNPS is added to the dictionary!\n",
      "INTU is added to the dictionary!\n",
      "TYL is added to the dictionary!\n",
      "ADBE is added to the dictionary!\n",
      "ANSS is added to the dictionary!\n",
      "ANET is added to the dictionary!\n",
      "CSCO is added to the dictionary!\n",
      "MSI is added to the dictionary!\n",
      "FFIV is added to the dictionary!\n",
      "JNPR is added to the dictionary!\n",
      "APH is added to the dictionary!\n",
      "GLW is added to the dictionary!\n",
      "TDY is added to the dictionary!\n",
      "ROP is added to the dictionary!\n",
      "TRMB is added to the dictionary!\n",
      "KEYS is added to the dictionary!\n",
      "ZBRA is added to the dictionary!\n",
      "TEL is added to the dictionary!\n",
      "JBL is added to the dictionary!\n",
      "IT is added to the dictionary!\n",
      "CTSH is added to the dictionary!\n",
      "EPAM is added to the dictionary!\n",
      "ACN is added to the dictionary!\n",
      "IBM is added to the dictionary!\n",
      "VRSN is added to the dictionary!\n",
      "AKAM is added to the dictionary!\n",
      "TER is added to the dictionary!\n",
      "KLAC is added to the dictionary!\n",
      "LRCX is added to the dictionary!\n",
      "AMAT is added to the dictionary!\n",
      "ENPH is added to the dictionary!\n",
      "QCOM is added to the dictionary!\n",
      "AVGO is added to the dictionary!\n",
      "SWKS is added to the dictionary!\n",
      "FSLR is added to the dictionary!\n",
      "QRVO is added to the dictionary!\n",
      "TXN is added to the dictionary!\n",
      "AMD is added to the dictionary!\n",
      "ADI is added to the dictionary!\n",
      "INTC is added to the dictionary!\n",
      "ON is added to the dictionary!\n",
      "NXPI is added to the dictionary!\n",
      "NVDA is added to the dictionary!\n",
      "MCHP is added to the dictionary!\n",
      "MU is added to the dictionary!\n",
      "MPWR is added to the dictionary!\n",
      "GEN is added to the dictionary!\n",
      "FTNT is added to the dictionary!\n",
      "NOW is added to the dictionary!\n",
      "MSFT is added to the dictionary!\n",
      "PANW is added to the dictionary!\n",
      "CDW is added to the dictionary!\n",
      "NTAP is added to the dictionary!\n",
      "STX is added to the dictionary!\n",
      "HPQ is added to the dictionary!\n",
      "WDC is added to the dictionary!\n",
      "AAPL is added to the dictionary!\n",
      "HPE is added to the dictionary!\n"
     ]
    }
   ],
   "source": [
    "company_dict = {}\n",
    "\n",
    "for ticker in data.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(data[ticker].values.reshape(-1,1)))\n",
    "    company_dict[ticker] = {'scaler': scaler, 'scaled_data': scaled_data}\n",
    "    \n",
    "    print(f\"{ticker} is added to the dictionary!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scaler': MinMaxScaler(),\n",
       " 'scaled_data':              0\n",
       " 0     0.251273\n",
       " 1     0.237310\n",
       " 2     0.247831\n",
       " 3     0.249673\n",
       " 4     0.261550\n",
       " ...        ...\n",
       " 1001  0.517574\n",
       " 1002  0.528773\n",
       " 1003  0.528967\n",
       " 1004  0.527997\n",
       " 1005  0.521065\n",
       " \n",
       " [1006 rows x 1 columns]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_dict[\"ADSK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(data):\n",
    "    time_step = 22\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data.iloc[i : (i+time_step)])\n",
    "        y.append(data.iloc[i+time_step])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(984, 22)\n",
      "(984,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([374.35998535, 371.3500061 , 379.3999939 , 376.55999756,\n",
       "       373.92001343, 369.86999512, 375.04000854, 375.94000244,\n",
       "       386.25      , 388.94000244, 384.92999268, 378.35998535,\n",
       "       375.92001343, 379.98999023, 379.3999939 , 376.51998901,\n",
       "       381.55999756, 381.67999268, 380.19000244, 380.42999268,\n",
       "       378.76998901, 377.55999756, 377.26000977, 375.67001343,\n",
       "       364.52999878, 365.1000061 , 368.39001465, 371.32000732,\n",
       "       371.61999512, 368.82998657, 368.        , 366.61999512,\n",
       "       371.55999756, 371.73999023, 366.95001221, 370.11999512,\n",
       "       367.95999146, 368.83999634, 373.27999878, 375.14001465,\n",
       "       377.07998657, 409.70999146, 408.5       , 401.67999268,\n",
       "       393.08999634, 395.39001465, 434.20001221, 444.73001099,\n",
       "       464.82998657, 454.95999146, 454.54998779, 449.35998535,\n",
       "       450.16000366, 443.83999634, 435.32000732, 437.69000244,\n",
       "       438.77999878, 446.04000854, 449.98001099, 446.04998779,\n",
       "       446.36999512, 441.19000244, 435.5       , 426.33999634,\n",
       "       426.13000488, 419.55999756, 417.20999146, 426.77999878,\n",
       "       427.1499939 , 427.8999939 , 435.41000366, 434.01000977,\n",
       "       432.73999023, 431.10998535, 431.29000854, 445.72000122,\n",
       "       438.70001221, 441.3500061 , 448.83999634, 454.10998535,\n",
       "       461.17999268, 462.92999268, 457.32998657, 451.70999146,\n",
       "       454.        , 450.75      , 456.45001221, 449.32998657,\n",
       "       448.73999023, 450.8999939 , 451.79998779, 453.51000977,\n",
       "       441.17999268, 440.11999512, 441.42001343, 444.        ,\n",
       "       435.98999023, 428.95999146, 431.39001465, 428.45999146,\n",
       "       434.8500061 , 436.73999023, 428.20999146, 424.1000061 ,\n",
       "       426.01998901, 436.47000122, 436.39001465, 448.23999023,\n",
       "       435.85998535, 442.23999023, 443.67999268, 451.57000732,\n",
       "       457.27999878, 458.89001465, 460.45001221, 466.8500061 ,\n",
       "       465.69000244, 458.08999634, 459.14001465, 461.82000732,\n",
       "       454.35998535, 466.85998535, 466.1499939 , 451.92999268,\n",
       "       460.01000977, 460.01998901, 453.27999878, 444.26998901,\n",
       "       446.8500061 , 447.82998657, 444.3500061 , 446.70999146,\n",
       "       460.70999146, 458.97000122, 463.82000732, 450.72000122,\n",
       "       461.32000732, 459.75      , 473.26000977, 479.92001343,\n",
       "       490.10998535, 494.95001221, 496.23001099, 489.67001343,\n",
       "       491.95999146, 492.04998779, 485.73001099, 482.36999512,\n",
       "       467.64001465, 467.82000732, 468.02999878, 455.26000977,\n",
       "       453.54000854, 457.        , 460.94000244, 469.44000244,\n",
       "       475.38000488, 478.64001465, 487.94000244, 489.94000244,\n",
       "       499.01998901, 506.16000366, 505.17001343, 518.83001709,\n",
       "       522.88000488, 538.32000732, 529.32000732, 533.45001221,\n",
       "       534.7800293 , 541.52001953, 540.38000488, 541.0300293 ,\n",
       "       542.69000244, 543.5300293 , 543.72998047, 552.46002197,\n",
       "       543.22998047, 545.96002197, 531.20001221, 535.71002197,\n",
       "       527.48999023, 534.15002441, 535.92999268, 556.02001953,\n",
       "       567.05999756, 568.09002686, 551.45001221, 556.27001953,\n",
       "       559.69000244, 558.65002441, 551.7199707 , 559.96002197,\n",
       "       524.46002197, 520.25      , 518.09997559, 517.40997314,\n",
       "       514.90997314])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = rolling_window(data[\"SNPS\"])\n",
    "y[-197:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(984, 22, 1)\n",
      "(984, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.0994974])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = rolling_window(company_dict[\"SNPS\"][\"scaled_data\"])\n",
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (787, 22, 1)\n",
      "y_train: (787, 1)\n",
      "x_test: (197, 22, 1)\n",
      "y_test: (197, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[374.35998535],\n",
       "       [371.3500061 ],\n",
       "       [379.3999939 ],\n",
       "       [376.55999756],\n",
       "       [373.92001343],\n",
       "       [369.86999512],\n",
       "       [375.04000854],\n",
       "       [375.94000244],\n",
       "       [386.25      ],\n",
       "       [388.94000244],\n",
       "       [384.92999268],\n",
       "       [378.35998535],\n",
       "       [375.92001343],\n",
       "       [379.98999023],\n",
       "       [379.3999939 ],\n",
       "       [376.51998901],\n",
       "       [381.55999756],\n",
       "       [381.67999268],\n",
       "       [380.19000244],\n",
       "       [380.42999268],\n",
       "       [378.76998901],\n",
       "       [377.55999756],\n",
       "       [377.26000977],\n",
       "       [375.67001343],\n",
       "       [364.52999878],\n",
       "       [365.1000061 ],\n",
       "       [368.39001465],\n",
       "       [371.32000732],\n",
       "       [371.61999512],\n",
       "       [368.82998657],\n",
       "       [368.        ],\n",
       "       [366.61999512],\n",
       "       [371.55999756],\n",
       "       [371.73999023],\n",
       "       [366.95001221],\n",
       "       [370.11999512],\n",
       "       [367.95999146],\n",
       "       [368.83999634],\n",
       "       [373.27999878],\n",
       "       [375.14001465],\n",
       "       [377.07998657],\n",
       "       [409.70999146],\n",
       "       [408.5       ],\n",
       "       [401.67999268],\n",
       "       [393.08999634],\n",
       "       [395.39001465],\n",
       "       [434.20001221],\n",
       "       [444.73001099],\n",
       "       [464.82998657],\n",
       "       [454.95999146],\n",
       "       [454.54998779],\n",
       "       [449.35998535],\n",
       "       [450.16000366],\n",
       "       [443.83999634],\n",
       "       [435.32000732],\n",
       "       [437.69000244],\n",
       "       [438.77999878],\n",
       "       [446.04000854],\n",
       "       [449.98001099],\n",
       "       [446.04998779],\n",
       "       [446.36999512],\n",
       "       [441.19000244],\n",
       "       [435.5       ],\n",
       "       [426.33999634],\n",
       "       [426.13000488],\n",
       "       [419.55999756],\n",
       "       [417.20999146],\n",
       "       [426.77999878],\n",
       "       [427.1499939 ],\n",
       "       [427.8999939 ],\n",
       "       [435.41000366],\n",
       "       [434.01000977],\n",
       "       [432.73999023],\n",
       "       [431.10998535],\n",
       "       [431.29000854],\n",
       "       [445.72000122],\n",
       "       [438.70001221],\n",
       "       [441.3500061 ],\n",
       "       [448.83999634],\n",
       "       [454.10998535],\n",
       "       [461.17999268],\n",
       "       [462.92999268],\n",
       "       [457.32998657],\n",
       "       [451.70999146],\n",
       "       [454.        ],\n",
       "       [450.75      ],\n",
       "       [456.45001221],\n",
       "       [449.32998657],\n",
       "       [448.73999023],\n",
       "       [450.8999939 ],\n",
       "       [451.79998779],\n",
       "       [453.51000977],\n",
       "       [441.17999268],\n",
       "       [440.11999512],\n",
       "       [441.42001343],\n",
       "       [444.        ],\n",
       "       [435.98999023],\n",
       "       [428.95999146],\n",
       "       [431.39001465],\n",
       "       [428.45999146],\n",
       "       [434.8500061 ],\n",
       "       [436.73999023],\n",
       "       [428.20999146],\n",
       "       [424.1000061 ],\n",
       "       [426.01998901],\n",
       "       [436.47000122],\n",
       "       [436.39001465],\n",
       "       [448.23999023],\n",
       "       [435.85998535],\n",
       "       [442.23999023],\n",
       "       [443.67999268],\n",
       "       [451.57000732],\n",
       "       [457.27999878],\n",
       "       [458.89001465],\n",
       "       [460.45001221],\n",
       "       [466.8500061 ],\n",
       "       [465.69000244],\n",
       "       [458.08999634],\n",
       "       [459.14001465],\n",
       "       [461.82000732],\n",
       "       [454.35998535],\n",
       "       [466.85998535],\n",
       "       [466.1499939 ],\n",
       "       [451.92999268],\n",
       "       [460.01000977],\n",
       "       [460.01998901],\n",
       "       [453.27999878],\n",
       "       [444.26998901],\n",
       "       [446.8500061 ],\n",
       "       [447.82998657],\n",
       "       [444.3500061 ],\n",
       "       [446.70999146],\n",
       "       [460.70999146],\n",
       "       [458.97000122],\n",
       "       [463.82000732],\n",
       "       [450.72000122],\n",
       "       [461.32000732],\n",
       "       [459.75      ],\n",
       "       [473.26000977],\n",
       "       [479.92001343],\n",
       "       [490.10998535],\n",
       "       [494.95001221],\n",
       "       [496.23001099],\n",
       "       [489.67001343],\n",
       "       [491.95999146],\n",
       "       [492.04998779],\n",
       "       [485.73001099],\n",
       "       [482.36999512],\n",
       "       [467.64001465],\n",
       "       [467.82000732],\n",
       "       [468.02999878],\n",
       "       [455.26000977],\n",
       "       [453.54000854],\n",
       "       [457.        ],\n",
       "       [460.94000244],\n",
       "       [469.44000244],\n",
       "       [475.38000488],\n",
       "       [478.64001465],\n",
       "       [487.94000244],\n",
       "       [489.94000244],\n",
       "       [499.01998901],\n",
       "       [506.16000366],\n",
       "       [505.17001343],\n",
       "       [518.83001709],\n",
       "       [522.88000488],\n",
       "       [538.32000732],\n",
       "       [529.32000732],\n",
       "       [533.45001221],\n",
       "       [534.7800293 ],\n",
       "       [541.52001953],\n",
       "       [540.38000488],\n",
       "       [541.0300293 ],\n",
       "       [542.69000244],\n",
       "       [543.5300293 ],\n",
       "       [543.72998047],\n",
       "       [552.46002197],\n",
       "       [543.22998047],\n",
       "       [545.96002197],\n",
       "       [531.20001221],\n",
       "       [535.71002197],\n",
       "       [527.48999023],\n",
       "       [534.15002441],\n",
       "       [535.92999268],\n",
       "       [556.02001953],\n",
       "       [567.05999756],\n",
       "       [568.09002686],\n",
       "       [551.45001221],\n",
       "       [556.27001953],\n",
       "       [559.69000244],\n",
       "       [558.65002441],\n",
       "       [551.7199707 ],\n",
       "       [559.96002197],\n",
       "       [524.46002197],\n",
       "       [520.25      ],\n",
       "       [518.09997559],\n",
       "       [517.40997314],\n",
       "       [514.90997314]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "print(f\"x_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "print(f\"x_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "y_test_inverse = company_dict[\"SNPS\"][\"scaler\"].inverse_transform(y_test)\n",
    "y_test_inverse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingWindowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float), torch.tensor(self.y[idx], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, num_layers = layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device) \n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device) \n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = RollingWindowDataset(X_train, y_train)\n",
    "test_data = RollingWindowDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_dim=X.shape[2], hidden_dim=22, layer_dim=1, output_dim=y_train.shape[1]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(config):\n",
    "\n",
    "    batch_size = int(round(config[\"batch_size\"]))\n",
    "    epochs = int(round(config[\"epochs\"]))\n",
    "    hidden_dim = int(round(config[\"hidden_dim\"]))\n",
    "    layer_dim = int(round(config[\"layer_dim\"]))\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    model = LSTMModel(input_dim=X.shape[2], hidden_dim=hidden_dim, layer_dim=layer_dim, output_dim=y_train.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('epochs {}/{}'.format(epoch+1,epochs))\n",
    "\n",
    "        running_loss = .0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # Clear the gradients\n",
    "            output = model(data)  # Forward pass\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward() # Compute gradient\n",
    "\n",
    "            optimizer.step() # Update params\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        train_loss = running_loss/len(train_loader)\n",
    "        ray.train.report({\"mse\": train_loss})\n",
    "        \n",
    "        print(f'train_loss {train_loss}')\n",
    "            # if batch_idx % 10 == 0:\n",
    "            #     print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test(batch_size,test):\n",
    "    all_predictions = []\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    running_loss = .0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(data)\n",
    "            loss = criterion(preds,target)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "            \n",
    "        test_loss = running_loss/len(test_loader)\n",
    "\n",
    "        print(f'test_loss {test_loss}')\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    return all_predictions  # Return the NumPy array of predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 22:30:28,303\tINFO tune.py:583 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2024-01-15 22:30:28,329\tWARNING bayesopt_search.py:431 -- BayesOpt does not support specific sampling methods. The LogUniform sampler will be dropped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-01-15 22:30:28 (running for 00:00:00.12)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 1/30 (1 PENDING)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 1/146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th style=\"text-align: right;\">        mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lstm_0204f2e4</td><td style=\"text-align: right;\">0.0383491  </td></tr>\n",
       "<tr><td>train_lstm_1e4479bc</td><td style=\"text-align: right;\">0.0002396  </td></tr>\n",
       "<tr><td>train_lstm_2d498e46</td><td style=\"text-align: right;\">0.0367006  </td></tr>\n",
       "<tr><td>train_lstm_373ac995</td><td style=\"text-align: right;\">0.0347314  </td></tr>\n",
       "<tr><td>train_lstm_3e518c31</td><td style=\"text-align: right;\">0.0238759  </td></tr>\n",
       "<tr><td>train_lstm_4ae6ab84</td><td style=\"text-align: right;\">0.0134791  </td></tr>\n",
       "<tr><td>train_lstm_5d74cb28</td><td style=\"text-align: right;\">0.0378146  </td></tr>\n",
       "<tr><td>train_lstm_7027d030</td><td style=\"text-align: right;\">0.0276636  </td></tr>\n",
       "<tr><td>train_lstm_7147d8c2</td><td style=\"text-align: right;\">0.058548   </td></tr>\n",
       "<tr><td>train_lstm_7557d3d0</td><td style=\"text-align: right;\">0.0714388  </td></tr>\n",
       "<tr><td>train_lstm_7ab93f4c</td><td style=\"text-align: right;\">0.077854   </td></tr>\n",
       "<tr><td>train_lstm_841d1191</td><td style=\"text-align: right;\">0.0364806  </td></tr>\n",
       "<tr><td>train_lstm_87cc0b71</td><td style=\"text-align: right;\">0.0446934  </td></tr>\n",
       "<tr><td>train_lstm_92a955a7</td><td style=\"text-align: right;\">0.0681622  </td></tr>\n",
       "<tr><td>train_lstm_931aae89</td><td style=\"text-align: right;\">0.0265293  </td></tr>\n",
       "<tr><td>train_lstm_99d06cb7</td><td style=\"text-align: right;\">0.0263163  </td></tr>\n",
       "<tr><td>train_lstm_aff29304</td><td style=\"text-align: right;\">0.0376343  </td></tr>\n",
       "<tr><td>train_lstm_b762e6f9</td><td style=\"text-align: right;\">0.0241873  </td></tr>\n",
       "<tr><td>train_lstm_b7d93e97</td><td style=\"text-align: right;\">0.0176448  </td></tr>\n",
       "<tr><td>train_lstm_c468d14d</td><td style=\"text-align: right;\">0.0511783  </td></tr>\n",
       "<tr><td>train_lstm_c8b1f01a</td><td style=\"text-align: right;\">0.0118143  </td></tr>\n",
       "<tr><td>train_lstm_cbb3f287</td><td style=\"text-align: right;\">0.0168999  </td></tr>\n",
       "<tr><td>train_lstm_cf2beb8d</td><td style=\"text-align: right;\">0.015125   </td></tr>\n",
       "<tr><td>train_lstm_cfee6e8a</td><td style=\"text-align: right;\">0.0286102  </td></tr>\n",
       "<tr><td>train_lstm_e0854834</td><td style=\"text-align: right;\">0.0142048  </td></tr>\n",
       "<tr><td>train_lstm_e8bff484</td><td style=\"text-align: right;\">0.000536117</td></tr>\n",
       "<tr><td>train_lstm_ed8ae13f</td><td style=\"text-align: right;\">0.102823   </td></tr>\n",
       "<tr><td>train_lstm_ef97d5c8</td><td style=\"text-align: right;\">0.128935   </td></tr>\n",
       "<tr><td>train_lstm_f51c642a</td><td style=\"text-align: right;\">0.0421469  </td></tr>\n",
       "<tr><td>train_lstm_f99bf79c</td><td style=\"text-align: right;\">0.101528   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.011323432127634684\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 2/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.049105454119853675\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 3/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.01875353842236412\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 4/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.023409905707618844\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 5/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.025049516077463824\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 6/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.021086348531146843\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 7/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0202363688343515\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 8/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0166974315555611\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 9/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.009520210088036643\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 10/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.002958080600365065\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 11/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0033274217372915396\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 12/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.007151067644978563\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 13/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.007508625096913117\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 14/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0022978793349466287\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 15/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0026822820461044707\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 16/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.003536394540181694\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 17/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0029724396493596337\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 18/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0029262567392531005\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 19/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0019522938625110935\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 20/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.002259002392141459\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 21/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0026998557489908612\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 22/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.001771392145504554\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 23/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0024781236328029386\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 24/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0029344108334043995\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 25/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.001698914270188349\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 26/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.002241057275872057\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 27/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0026369171537226066\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 28/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0017696798652953778\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 29/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.002373590291729973\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 30/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.002680105602242596\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 31/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0021585346136513786\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 32/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.002947770403504061\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 33/146\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:30:33 (running for 00:00:05.14)\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.011323432127634684\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 2/30 (1 PENDING, 1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.003271959382497395\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 34/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0035110523022012785\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 35/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0036358592260512523\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 36/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.004530347709078342\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 37/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.005074725777376443\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 38/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0022052962291733516\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 39/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.003908588201738894\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 40/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0030691818004318825\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 41/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0015735391583196663\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 42/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.00248359739149843\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 43/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.002141861892596353\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 44/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0012556730895691242\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 45/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0018315863174696763\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 46/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.001816024040939131\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 47/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.001177042455916914\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 48/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0015874420835946996\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 49/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0016773510639419935\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 50/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0011407799514320989\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 51/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0014644757854208972\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 52/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0016095835987168055\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 53/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.001080637208360713\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 54/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0013458461908157915\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 55/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0015458151489535037\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 56/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0009799998661037534\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 57/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.001181805838617341\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 58/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.001425079795202085\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 59/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0008499077521264553\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 60/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0009776595143193845\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 61/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0012227041370351799\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 62/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0007331746122266244\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 63/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0007907504271618867\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 64/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0009823381988098845\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 65/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006928371124862073\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 66/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006785592795495177\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 67/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0007831822852798117\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 68/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0007384837275215735\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 69/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.00067665658207261\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 70/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006847951311404662\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 71/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006985451958219832\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 72/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0007019735736927638\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 73/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006898133433423936\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 74/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006674483156530187\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 75/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006583056056115311\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 76/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006620081585424487\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 77/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006685285516141448\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 78/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006642429592223683\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 79/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006492211044436166\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 80/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006364302219784198\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 81/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006327257106022444\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 82/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006348873503156938\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 83/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006339302502359109\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 84/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006261596063268371\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 85/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006159756897735255\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 86/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006083893058530521\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 87/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006040028771773601\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 88/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0006003429480188061\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 89/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005954303208757968\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 90/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005893494671909139\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 91/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005830239303274235\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 92/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005770431428876085\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 93/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005714356169240394\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 94/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.000566088805499021\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 95/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005608707081895167\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 96/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005557534289740337\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 97/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005506914276338648\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 98/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005457301097825015\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 99/146\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m train_loss 0.0005408499127952382\n",
      "\u001b[36m(train_lstm pid=44266)\u001b[0m epochs 100/146\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:30:38 (running for 00:00:10.21)\n",
      "Using AsyncHyperBand: num_stopped=1\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.011323432127634684\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 2/30 (1 PENDING, 1 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44331)\u001b[0m epochs 1/56\n",
      "\u001b[36m(train_lstm pid=44331)\u001b[0m train_loss 0.005087230403339216\n",
      "\u001b[36m(train_lstm pid=44331)\u001b[0m epochs 2/56\n",
      "\u001b[36m(train_lstm pid=44331)\u001b[0m train_loss 0.07219844837056692\n",
      "\u001b[36m(train_lstm pid=44331)\u001b[0m epochs 3/56\n",
      "\u001b[36m(train_lstm pid=44331)\u001b[0m train_loss 0.021749373951666606\n",
      "\u001b[36m(train_lstm pid=44331)\u001b[0m epochs 4/56\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:30:43 (running for 00:00:15.26)\n",
      "Using AsyncHyperBand: num_stopped=2\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.034597984998767706 | Iter 1.000: -0.006646280834413083\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 3/30 (1 PENDING, 2 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:30:48 (running for 00:00:20.30)\n",
      "Using AsyncHyperBand: num_stopped=3\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.034597984998767706 | Iter 1.000: -0.00820533126548695\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 4/30 (1 PENDING, 3 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44443)\u001b[0m epochs 1/81\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:30:53 (running for 00:00:25.34)\n",
      "Using AsyncHyperBand: num_stopped=4\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.034597984998767706 | Iter 1.000: -0.009764381696560817\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 6/30 (1 PENDING, 1 RUNNING, 4 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44542)\u001b[0m epochs 1/70\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:30:58 (running for 00:00:30.36)\n",
      "Using AsyncHyperBand: num_stopped=6\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.034597984998767706 | Iter 1.000: -0.011862337769950196\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 7/30 (1 PENDING, 6 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:03 (running for 00:00:35.46)\n",
      "Using AsyncHyperBand: num_stopped=7\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.034597984998767706 | Iter 1.000: -0.012401243412265709\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 8/30 (1 PENDING, 7 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44647)\u001b[0m epochs 1/81\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:08 (running for 00:00:40.56)\n",
      "Using AsyncHyperBand: num_stopped=8\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.034597984998767706 | Iter 1.000: -0.012940149054581221\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 9/30 (1 PENDING, 8 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44760)\u001b[0m epochs 1/81\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:13 (running for 00:00:45.57)\n",
      "Using AsyncHyperBand: num_stopped=9\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.034597984998767706 | Iter 1.000: -0.013479054696896734\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 10/30 (1 RUNNING, 9 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44808)\u001b[0m train_loss 0.010079943555562446\n",
      "\u001b[36m(train_lstm pid=44808)\u001b[0m train_loss 0.048203651851508766\n",
      "\u001b[36m(train_lstm pid=44808)\u001b[0m train_loss 0.016184284817427397\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:18 (running for 00:00:50.64)\n",
      "Using AsyncHyperBand: num_stopped=11\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.030612254864536226 | Iter 1.000: -0.012401243412265709\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 12/30 (1 PENDING, 11 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44859)\u001b[0m epochs 1/149\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:24 (running for 00:00:55.72)\n",
      "Using AsyncHyperBand: num_stopped=12\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.030612254864536226 | Iter 1.000: -0.012940149054581221\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 13/30 (1 PENDING, 12 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:29 (running for 00:01:00.73)\n",
      "Using AsyncHyperBand: num_stopped=13\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.003536394540181694 | Iter 4.000: -0.030612254864536226 | Iter 1.000: -0.013479054696896734\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 14/30 (1 PENDING, 13 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m epochs 1/149\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.012066564871929586\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.04197931297433873\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.02672824964004879\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.027197461284231395\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.024419128603767604\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.02684921744124343\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.019855565134397086\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.027486499786997836\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.025763793014145147\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.02274856004320706\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.01800653610068063\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.015769531736926485\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.01834343004641899\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.003932527722402786\n",
      "\u001b[36m(train_lstm pid=44970)\u001b[0m train_loss 0.08881031498215937\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:34 (running for 00:01:05.77)\n",
      "Using AsyncHyperBand: num_stopped=14\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.005605861053481931 | Iter 4.000: -0.026250572390078258 | Iter 1.000: -0.012419687328171372\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 15/30 (1 PENDING, 14 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:39 (running for 00:01:10.78)\n",
      "Using AsyncHyperBand: num_stopped=15\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.005605861053481931 | Iter 4.000: -0.026250572390078258 | Iter 1.000: -0.012772809784413159\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 17/30 (1 PENDING, 1 RUNNING, 15 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45080)\u001b[0m epochs 1/150\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:44 (running for 00:01:15.86)\n",
      "Using AsyncHyperBand: num_stopped=17\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.005605861053481931 | Iter 4.000: -0.026250572390078258 | Iter 1.000: -0.013479054696896734\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 18/30 (1 PENDING, 17 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m epochs 1/142\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.010477010268144883\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.04302573819698479\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.011951928828448918\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.022216627222055998\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.01008347077563835\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.011298838967027573\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.006692110175768344\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0018757611128071754\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.007183736305495796\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.002815111235787089\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0034944297714481275\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.000543035220918962\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0006512285438545335\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0004241078624125713\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00035279389871553017\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003423386565723027\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003102337779217543\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00028502105571813165\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002804018714903997\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00028356593583549303\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002954009178210981\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003161774410937841\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003490781550331471\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0004003802700586115\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00047835300098925544\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0005976296866160388\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0007758204989771072\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.001027827660996431\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0013469893114793545\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0016714529069409205\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0018382902545944\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00160634189132207\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0009883007072718241\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00045858257306882966\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00028611857067936886\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002980525221993538\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003072522774276037\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002770046194200404\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00024974492099807743\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00024030311396927573\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002393178404487956\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00024235751194422707\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002475946614420256\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00025428445895578567\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:49 (running for 00:01:20.89)\n",
      "Using AsyncHyperBand: num_stopped=17\n",
      "Bracket: Iter 64.000: -0.0009823381988098845 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.012419687328171372\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 19/30 (1 PENDING, 1 RUNNING, 17 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00026207421279440704\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00027069955215735646\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002802602231475668\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002909154959175234\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.000303049656325199\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00031721549972784345\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00033416620415384666\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00035490726328974304\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.000380725679860916\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0004132388365598252\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0004543643719024168\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0005061665914791564\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0005704015622121425\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.000647566779942001\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0007352220832567232\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0008255653044146199\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0009028253664907354\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0009424397402863877\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0009163174243160308\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0008088159632349673\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.000637629154460648\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0004556329826860187\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00032041531421082956\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00025969724895101256\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.000263229582812458\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002960684827345316\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00031945213469533395\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003124564626397422\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00028097263174668816\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002447848394065379\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00021886986500108973\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00020829731231340422\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00021234791067679628\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00022815115209176348\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002523826571212759\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00028167245293913694\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00031269615395173716\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003426867999503604\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003703016028829062\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0003956336653433167\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00041932424825902743\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00044167104799551173\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0004623742895021748\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00048060615461928625\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0004951249967024734\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0005043284926124705\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0005064029022693061\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0004996189784903366\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00048281633252582443\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.000455899995778544\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00042023790201011044\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00037872143170366494\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00033544489717594563\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.0002950507131082794\n",
      "\u001b[36m(train_lstm pid=45176)\u001b[0m train_loss 0.00026194837482762523\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:54 (running for 00:01:25.95)\n",
      "Using AsyncHyperBand: num_stopped=18\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.012419687328171372\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 19/30 (1 PENDING, 18 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45245)\u001b[0m epochs 1/143\u001b[32m [repeated 100x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:31:59 (running for 00:01:30.97)\n",
      "Using AsyncHyperBand: num_stopped=19\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.012772809784413159\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 20/30 (1 RUNNING, 19 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45365)\u001b[0m epochs 1/145\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:04 (running for 00:01:36.05)\n",
      "Using AsyncHyperBand: num_stopped=21\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.013479054696896734\n",
      "Logical resource usage: 0/8 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 22/30 (1 PENDING, 21 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:09 (running for 00:01:41.07)\n",
      "Using AsyncHyperBand: num_stopped=22\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.013660482621162373\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 23/30 (1 PENDING, 22 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45461)\u001b[0m epochs 1/151\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:14 (running for 00:01:46.16)\n",
      "Using AsyncHyperBand: num_stopped=23\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.01384191054542801\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 24/30 (1 PENDING, 23 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:19 (running for 00:01:51.18)\n",
      "Using AsyncHyperBand: num_stopped=24\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.023409905707618844 | Iter 1.000: -0.01402333846969365\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 26/30 (1 PENDING, 1 RUNNING, 24 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45575)\u001b[0m epochs 1/146\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_lstm pid=45575)\u001b[0m train_loss 0.01174845648711198\n",
      "\u001b[36m(train_lstm pid=45575)\u001b[0m train_loss 0.029126375447958708\n",
      "\u001b[36m(train_lstm pid=45575)\u001b[0m train_loss 0.025060014231712557\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:24 (running for 00:01:56.22)\n",
      "Using AsyncHyperBand: num_stopped=26\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.02435679460177198 | Iter 1.000: -0.013660482621162373\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 27/30 (1 PENDING, 26 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45672)\u001b[0m epochs 1/146\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[36m(train_lstm pid=45672)\u001b[0m train_loss 0.01133741049610156\n",
      "\u001b[36m(train_lstm pid=45672)\u001b[0m train_loss 0.0776446872332599\n",
      "\u001b[36m(train_lstm pid=45672)\u001b[0m train_loss 0.00799452566813367\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:29 (running for 00:02:01.29)\n",
      "Using AsyncHyperBand: num_stopped=27\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.02530368349592512 | Iter 1.000: -0.012772809784413159\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 28/30 (1 PENDING, 27 TERMINATED)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:34 (running for 00:02:06.29)\n",
      "Using AsyncHyperBand: num_stopped=28\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.02530368349592512 | Iter 1.000: -0.013125932240654947\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 29/30 (1 PENDING, 28 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45786)\u001b[0m epochs 1/145\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:39 (running for 00:02:11.35)\n",
      "Using AsyncHyperBand: num_stopped=29\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.02530368349592512 | Iter 1.000: -0.013479054696896734\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 30/30 (1 RUNNING, 29 TERMINATED)\n",
      "\n",
      "\n",
      "\u001b[36m(train_lstm pid=45840)\u001b[0m train_loss 0.007433502178173512\n",
      "\u001b[36m(train_lstm pid=45840)\u001b[0m train_loss 0.06986573248286732\n",
      "== Status ==\n",
      "Current time: 2024-01-15 22:32:40 (running for 00:02:12.04)\n",
      "Using AsyncHyperBand: num_stopped=30\n",
      "Bracket: Iter 64.000: -0.0008521965221286966 | Iter 16.000: -0.0019393665983769982 | Iter 4.000: -0.026250572390078258 | Iter 1.000: -0.012419687328171372\n",
      "Logical resource usage: 1.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/arda/ray_results/train_lstm_2024-01-15_22-30-28\n",
      "Number of trials: 30/30 (30 TERMINATED)\n",
      "+---------------------+------------+---------------------+--------------+----------+--------------+-------------+-----------------+-------------+----------------------+\n",
      "| Trial name          | status     | loc                 |   batch_size |   epochs |   hidden_dim |   layer_dim |   learning_rate |         mse |   training_iteration |\n",
      "|---------------------+------------+---------------------+--------------+----------+--------------+-------------+-----------------+-------------+----------------------|\n",
      "| train_lstm_e8bff484 | TERMINATED | 192.168.1.157:44266 |      68.3304 | 146.022  |      61.5277 |     2.79598 |     0.00164458  | 0.000536117 |                  100 |\n",
      "| train_lstm_92a955a7 | TERMINATED | 192.168.1.157:44331 |      47.1315 |  55.8664 |      68.7735 |     2.80335 |     0.00710992  | 0.0681622   |                    4 |\n",
      "| train_lstm_4ae6ab84 | TERMINATED | 192.168.1.157:44387 |      33.9967 | 147.961  |      66.9519 |     1.63702 |     0.00190007  | 0.0134791   |                    1 |\n",
      "| train_lstm_e0854834 | TERMINATED | 192.168.1.157:44443 |      49.7902 |  80.7285 |      50.3368 |     2.29584 |     0.00298317  | 0.0142048   |                    1 |\n",
      "| train_lstm_99d06cb7 | TERMINATED | 192.168.1.157:44496 |      91.3497 |  64.0889 |      37.7758 |     2.09909 |     0.00461509  | 0.0263163   |                    1 |\n",
      "| train_lstm_7557d3d0 | TERMINATED | 192.168.1.157:44542 |     108.162  |  70.1671 |      49.7687 |     2.77724 |     0.000559859 | 0.0714388   |                    1 |\n",
      "| train_lstm_7027d030 | TERMINATED | 192.168.1.157:44593 |      90.9319 |  67.2229 |      25.5128 |     3.84666 |     0.00965976  | 0.0276636   |                    1 |\n",
      "| train_lstm_7147d8c2 | TERMINATED | 192.168.1.157:44647 |     110.415  |  80.766  |      27.2743 |     3.0527  |     0.00445751  | 0.058548    |                    1 |\n",
      "| train_lstm_373ac995 | TERMINATED | 192.168.1.157:44705 |      43.8377 | 100.013  |      23.857  |     3.72796 |     0.00266192  | 0.0347314   |                    1 |\n",
      "| train_lstm_cfee6e8a | TERMINATED | 192.168.1.157:44760 |      96.2647 |  81.4828 |      50.0837 |     2.64013 |     0.00193006  | 0.0286102   |                    1 |\n",
      "| train_lstm_5d74cb28 | TERMINATED | 192.168.1.157:44808 |      71.4142 | 143.585  |      60.3628 |     2.25677 |     0.00522099  | 0.0378146   |                    4 |\n",
      "| train_lstm_cbb3f287 | TERMINATED | 192.168.1.157:44859 |      34.3301 | 148.555  |      65.9115 |     2.34103 |     0.00921059  | 0.0168999   |                    1 |\n",
      "| train_lstm_f99bf79c | TERMINATED | 192.168.1.157:44913 |      88.6484 |  54.8261 |      23.9336 |     1.31358 |     0.00240868  | 0.101528    |                    1 |\n",
      "| train_lstm_c8b1f01a | TERMINATED | 192.168.1.157:44970 |      65.7176 | 148.647  |      63.9491 |     3.40084 |     0.00610591  | 0.0118143   |                   16 |\n",
      "| train_lstm_cf2beb8d | TERMINATED | 192.168.1.157:45026 |      63.814  | 147.492  |      60.5538 |     1.69576 |     0.00661     | 0.015125    |                    1 |\n",
      "| train_lstm_b7d93e97 | TERMINATED | 192.168.1.157:45080 |      66.8467 | 149.806  |      57.9379 |     1.60279 |     0.00888154  | 0.0176448   |                    1 |\n",
      "| train_lstm_0204f2e4 | TERMINATED | 192.168.1.157:45124 |      69.5791 | 150.907  |      62.6195 |     3.18883 |     0.00420651  | 0.0383491   |                    1 |\n",
      "| train_lstm_1e4479bc | TERMINATED | 192.168.1.157:45176 |      64.6178 | 141.637  |      68.603  |     1.17112 |     0.00636682  | 0.0002396   |                  100 |\n",
      "| train_lstm_f51c642a | TERMINATED | 192.168.1.157:45245 |      66.1213 | 142.528  |      65.3019 |     2.3589  |     0.0010903   | 0.0421469   |                    1 |\n",
      "| train_lstm_ed8ae13f | TERMINATED | 192.168.1.157:45311 |      64.7396 | 142.181  |      63.8752 |     4       |     0.0001      | 0.102823    |                    1 |\n",
      "| train_lstm_ef97d5c8 | TERMINATED | 192.168.1.157:45365 |      65.1868 | 145.04   |      71.5217 |     3.33012 |     0.000614686 | 0.128935    |                    1 |\n",
      "| train_lstm_7ab93f4c | TERMINATED | 192.168.1.157:45407 |      63.126  | 141.162  |      70.2613 |     1.61136 |     0.000243797 | 0.077854    |                    1 |\n",
      "| train_lstm_2d498e46 | TERMINATED | 192.168.1.157:45461 |      65.1795 | 150.643  |      61.3968 |     2.85627 |     0.00102251  | 0.0367006   |                    1 |\n",
      "| train_lstm_b762e6f9 | TERMINATED | 192.168.1.157:45518 |      63.7042 | 148.669  |      61.4607 |     2.28812 |     0.00247652  | 0.0241873   |                    1 |\n",
      "| train_lstm_841d1191 | TERMINATED | 192.168.1.157:45575 |      33.5598 | 146.444  |      66.4502 |     1.30618 |     0.0083387   | 0.0364806   |                    4 |\n",
      "| train_lstm_931aae89 | TERMINATED | 192.168.1.157:45630 |      67.4799 | 146.464  |      59.41   |     3.09837 |     0.00657503  | 0.0265293   |                    1 |\n",
      "| train_lstm_c468d14d | TERMINATED | 192.168.1.157:45672 |      68.5823 | 145.733  |      63.185  |     2.40191 |     0.00356422  | 0.0511783   |                    4 |\n",
      "| train_lstm_aff29304 | TERMINATED | 192.168.1.157:45730 |      68.1042 | 147.913  |      64.3643 |     2.24904 |     0.00143649  | 0.0376343   |                    1 |\n",
      "| train_lstm_3e518c31 | TERMINATED | 192.168.1.157:45786 |      69.3668 | 145.255  |      60.9439 |     3.81726 |     0.00433165  | 0.0238759   |                    1 |\n",
      "| train_lstm_87cc0b71 | TERMINATED | 192.168.1.157:45840 |      69.0937 | 146.869  |      60.0516 |     3.05678 |     0.00282793  | 0.0446934   |                    4 |\n",
      "+---------------------+------------+---------------------+--------------+----------+--------------+-------------+-----------------+-------------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 22:32:40,382\tINFO tune.py:1042 -- Total run time: 132.08 seconds (132.04 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lstm pid=45840)\u001b[0m train_loss 0.01136684336718948\n",
      "Best config:  {'learning_rate': 0.006366818994423863, 'batch_size': 64.61777697625115, 'epochs': 141.6366927241757, 'hidden_dim': 68.603038592183, 'layer_dim': 1.1711154959957395}\n",
      "Best MSE: 0.00023960028864586583\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter configuration\n",
    "config = {\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"batch_size\": tune.uniform(32, 129),  # Convert to continuous\n",
    "    \"epochs\": tune.uniform(50, 151),      # Convert to continuous\n",
    "    \"hidden_dim\": tune.uniform(22, 76),   # Convert to continuous\n",
    "    \"layer_dim\": tune.uniform(1, 4)\n",
    "}\n",
    "\n",
    "# Define scheduler and reporter\n",
    "scheduler = ASHAScheduler(metric=\"mse\", mode=\"min\")\n",
    "reporter = CLIReporter(metric_columns=[\"mse\", \"training_iteration\"])\n",
    "bayesopt = BayesOptSearch(metric=\"mse\", mode=\"min\")\n",
    "\n",
    "\n",
    "# Start the tuning process\n",
    "analysis = tune.run(\n",
    "    train_lstm,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},  # adjust based on your GPU\n",
    "    config=config,\n",
    "    num_samples=30,\n",
    "    scheduler=scheduler,\n",
    "    search_alg=bayesopt,  # Use Bayesian optimization search\n",
    "    progress_reporter=reporter\n",
    ")\n",
    "\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_config = analysis.get_best_config(metric=\"mse\", mode=\"min\")\n",
    "best_mse = analysis.get_best_trial(metric=\"mse\", mode=\"min\").last_result[\"mse\"]\n",
    "\n",
    "print(\"Best config: \", best_config)\n",
    "print(f\"Best MSE: {best_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_lstm() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_lstm(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m30\u001b[39m, train_data)\n",
      "\u001b[0;31mTypeError\u001b[0m: train_lstm() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "train_lstm(32, 30, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = Test(32, test_data)\n",
    "\n",
    "preds_inverse = company_dict[\"SNPS\"][\"scaler\"].inverse_transform(preds)\n",
    "print(preds_inverse)\n",
    "\n",
    "mse = mean_squared_error(y_test_inverse, preds_inverse)\n",
    "mape = mean_absolute_percentage_error(y_test_inverse, preds_inverse)*100\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAPE: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_inverse = company_dict[\"SNPS\"][\"scaler\"].inverse_transform(y_test)\n",
    "preds_inverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = rolling_window(company_dict[\"CDNS\"][\"scaled_data\"])\n",
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "\n",
    "y_test_inverse = company_dict[\"CDNS\"][\"scaler\"].inverse_transform(y_test)\n",
    "y_test_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = RollingWindowDataset(X_train, y_train)\n",
    "test_data = RollingWindowDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = Test(32, test_data)\n",
    "preds_inverse = company_dict[\"CDNS\"][\"scaler\"].inverse_transform(preds)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test_inverse, preds_inverse)\n",
    "mape = mean_absolute_percentage_error(y_test_inverse, preds_inverse)*100\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAPE: {mape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
