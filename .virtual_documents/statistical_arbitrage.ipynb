import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error
import ray
from ray import tune
from ray.tune import CLIReporter
from ray.tune.schedulers import ASHAScheduler
from hyperopt import hp
from ray.tune.search.bayesopt import BayesOptSearch
from ray.tune.schedulers import ASHAScheduler
from functools import partial
from ray.experimental.tqdm_ray import tqdm
from ray.util.accelerators import NVIDIA_TESLA_V100

import torch
from ray import train, tune
from ray.tune.search.optuna import OptunaSearch
from ray import train

from sklearn.model_selection import TimeSeriesSplit


if torch.cuda.is_available():
    device = torch.device('cuda')
    print("gpu")
else:
    device = torch.device('cpu')
print(torch.__version__)
print('CUDA available:', torch.cuda.is_available())
print('CUDA version:', torch.version.cuda)
print('cuDNN version:', torch.backends.cudnn.version())


data = pd.read_csv("Technology_Firm_Stock_Price.csv")
data = data.sort_values(by="Date")
data.iloc[-197:]


data = data.drop(columns=["Date"])
data


def rolling_window(data):
    time_step = 22

    X, y = [], []

    for i in range(len(data) - time_step):
        X.append(data.iloc[i : (i+time_step)])
        y.append(data.iloc[i+time_step])
    
    X = np.array(X)
    y = np.array(y)

    print(X.shape)
    print(y.shape)

    return X, y


rolling_window(data["AAPL"])


company_dict = {}

for ticker in data.columns:
    scaler = MinMaxScaler(feature_range=(0,1))
    scaled_data = pd.DataFrame(scaler.fit_transform(data[ticker].values.reshape(-1,1)))

    X, y = rolling_window(scaled_data)
    print(X.shape)
    print(y.shape)

    company_dict[ticker] = {'scaler': scaler, 'scaled_data': scaled_data, 'X': X, 'y': y}
    
    print(f"{ticker} is added to the dictionary!")


company_dict["AAPL"]["X"]


X_train, X_test, y_train, y_test = train_test_split(company_dict["AAPL"]["X"], company_dict["AAPL"]["y"], test_size=0.2, shuffle=False)
X_train.shape
y_train.shape
print(f"x_train: {X_train.shape}")
print(f"y_train: {y_train.shape}")

print(f"x_test: {X_test.shape}")
print(f"y_test: {y_test.shape}")


class RollingWindowDataset(Dataset):
    def __init__(self, X, y, device="gpu"):
        self.X = torch.tensor(X, dtype=torch.float, device=device)
        self.y = torch.tensor(y, dtype=torch.float, device=device)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx].clone().detach().to(torch.float), self.y[idx].clone().detach().to(torch.float)



class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
        super(LSTMModel, self).__init__()

        self.hidden_dim = hidden_dim
        self.layer_dim = layer_dim

        self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, num_layers = layer_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # Initializing hidden state
        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device) 

        # Initialize cell state
        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device) 

        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        out = self.fc(out[:, -1, :])

        return out


train_data = RollingWindowDataset(X_train, y_train, device)
test_data = RollingWindowDataset(X_test, y_test, device)


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = LSTMModel(input_dim=X_train.shape[2], hidden_dim=22, layer_dim=1, output_dim=y_train.shape[1]).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.0001)
criterion = nn.MSELoss()


class LSTM_Trainer:
    def __init__(self, train_data, test_data, device, optimizer, criterion):
        self.train_data = train_data
        self.test_data = test_data
        self.device = device
        self.optimizer = optimizer
        self.criterion =  criterion
        self.model = LSTMModel(input_dim=1, hidden_dim=22, layer_dim=1, output_dim=1).to(self.device)

    def train_lstm(self, config, checkpoint_dir=None):

        batch_size = int(round(config["batch_size"]))
        epochs = int(round(config["epochs"]))
        hidden_dim = int(round(config["hidden_dim"]))
        layer_dim = int(round(config["layer_dim"]))
        learning_rate = config["learning_rate"]

        # Create the model
        self.model = LSTMModel(input_dim=1, hidden_dim=hidden_dim, layer_dim=layer_dim, output_dim=1).to(self.device)

        # Define loss and optimizer
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)

        # Define data loader (you may need to adapt this based on your data format)
        train_loader = DataLoader(self.train_data, batch_size=batch_size, shuffle=False)

        # Training loop
        for epoch in range(epochs):
            print('epochs {}/{}'.format(epoch+1,epochs))
            running_loss = .0
            self.model.train()


            for batch_idx, (data, target) in enumerate(train_loader):
                if isinstance(data, np.ndarray):
                    data = torch.from_numpy(data).float()
                if isinstance(target, np.ndarray):
                    target = torch.from_numpy(target).float()

                data, target = data.to(self.device), target.to(self.device)

                self.optimizer.zero_grad()
                predictions = self.model(data)
                loss = self.criterion(predictions, target)
                loss.backward()
                self.optimizer.step()

                running_loss += loss.item()


            train_loss = running_loss/len(train_loader)
            print(train_loss)
            train.report({'mse': train_loss})


    def Test(self, config, test_data):
        batch_size = int(round(config["batch_size"]))
        self.test_data = test_data

        all_predictions = []
        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)

        running_loss = .0
        
        self.model.eval()
        
        with torch.no_grad():

            for batch_idx, (data, target) in enumerate(test_loader):
                if isinstance(data, np.ndarray):
                    data = torch.from_numpy(data).float()
                if isinstance(target, np.ndarray):
                    target = torch.from_numpy(target).float()

                data, target = data.to(self.device), target.to(self.device)

                self.optimizer.zero_grad()
                preds = self.model(data)
                loss = self.criterion(preds,target)
                running_loss += loss.item()

                all_predictions.extend(preds.cpu().numpy())

                
            test_loss = running_loss/len(test_loader)

            print(f'test_loss {test_loss}')

        all_predictions = np.array(all_predictions)

        return all_predictions 
    
    
    def get_current_model(self):
        if self.model is not None:
            return self.model
        else:
            raise ValueError("Model has not been trained yet.")
        


def train_model_with_config(config, train_data, test_data, device, optimizer, criterion):
    trainer = LSTM_Trainer(train_data, test_data, device, optimizer, criterion)
    return trainer.train_lstm(config)


def tuning(train_data, test_data, device, optimizer, criterion):
    ray.shutdown()
    ray.init(num_cpus=8, num_gpus=1)

    # Define hyperparameter configuration
    config = {
        "learning_rate": tune.choice([1e-4, 1e-3, 1e-2]),
        "batch_size": tune.choice([32, 64, 128]),
        "epochs": tune.choice([50, 100, 150]),
        "hidden_dim": tune.choice([22, 35, 50, 75]),
        "layer_dim": tune.choice([1, 2, 3])
    }

    # Define scheduler and reporter
    scheduler = ASHAScheduler(
        metric="mse",
        mode="min",
        grace_period=15,  
        reduction_factor=2         
    )
    reporter = CLIReporter(metric_columns=["mse", "training_iteration"])

    # Optuna search algorithm
    optuna_search = OptunaSearch(metric="mse", mode="min")

    # Partial function for training with LSTM and given data
    #train_lstm_with_data = partial(trainer.train_lstm, device=trainer.device)

    trainable_with_cpu_gpu = tune.with_resources(lambda config: train_model_with_config(config, train_data, test_data, device, optimizer, criterion), {"cpu": 8, "gpu": 1})


    # Start the tuning process
    analysis = tune.run(
        trainable_with_cpu_gpu,
        # resources_per_trial={"cpu": 8, "gpu": 1},
        config=config,
        num_samples=50,
        scheduler=scheduler,
        search_alg=optuna_search,
        progress_reporter=None
    )

    # Get the best hyperparameters
    best_config = analysis.get_best_config(metric="mse", mode="min")
    best_mse = analysis.get_best_trial(metric="mse", mode="min").last_result["mse"]

    print("Best config: ", best_config)
    print(f"Best MSE: {best_mse}")

    return best_config


score = []
checked_compt = set()


for ticker in tqdm(data.columns, desc="Processing tickers"):

    comp_data_X = company_dict[ticker]["X"] 
    comp_data_y = company_dict[ticker]["y"] 
    X_train, X_test, y_train, y_test = train_test_split(comp_data_X, comp_data_y, test_size=0.2, shuffle=False)
    train_data = RollingWindowDataset(X_train, y_train, device)
    test_data = RollingWindowDataset(X_test, y_test, device)

    best_config = tuning(train_data, test_data, device, optimizer, criterion)

    trainer = LSTM_Trainer(train_data, test_data, device, optimizer, criterion)
    trainer.train_lstm(best_config)

    preds = trainer.Test(best_config, test_data)

    mape = mean_absolute_percentage_error(y_test, preds)*100

    for pair in data.columns:
        comp_data_X = company_dict[pair]["X"] 
        comp_data_y = company_dict[pair]["y"] 
        X_train, X_test, y_train, y_test = train_test_split(comp_data_X, comp_data_y, test_size=0.2, shuffle=False)

        test_data = RollingWindowDataset(X_test, y_test, device)

        preds = trainer.Test(best_config, test_data)

        pair_mape = mean_absolute_percentage_error(y_test, preds) * 100

        score.append((mape, ticker , pair, pair_mape))



score


similar_pairs = []
for pairs in score:
    mape_value = pairs[0]
    ticker = pairs[1]
    pair_ticker = pairs[2]
    pair_mape = pairs[3]

    if ticker != pair_ticker:
        mape_diff = abs(mape_value - pair_mape)
        similar_pairs.append((ticker, pair_ticker, mape_value, mape_diff, pair_mape))


sorted_similar_pairs = sorted(similar_pairs, key=lambda x: x[4])
sorted_similar_pairs


df = pd.DataFrame(sorted_similar_pairs, columns=['Ticker', "Pair Ticker", "Mape Value", 'MAPE_Diff', "Pair Mape"])
selected_ticker = df[df["Ticker"]=="AMAT" ]
selected_pair = selected_ticker[selected_ticker["Pair Ticker"] == "NXPI"]
selected_ticker.head(50)


selected_pair


df


df.to_csv("Pairs_Mape.csv", index=False) 



